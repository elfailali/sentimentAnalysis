{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05fa5ab4",
   "metadata": {},
   "source": [
    "# Clean the aug_emotion_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e1520d",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1dfc484d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Honor\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk ,re, string\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154ac992",
   "metadata": {},
   "source": [
    "# Read the dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "829d41ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"aug_emotion_data.csv\", index_col= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9809fb0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_content</th>\n",
       "      <th>sentiment_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT Your anxiety might have represented a crush...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>So not pumped for this interview nervous</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>so my boss told me today that I am on vacation...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>could feel humiliated bn detachment realized n...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Even in the 1930s the anti Semitism of literar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       clean_content  sentiment_id\n",
       "0  RT Your anxiety might have represented a crush...             0\n",
       "1           So not pumped for this interview nervous             0\n",
       "2  so my boss told me today that I am on vacation...             4\n",
       "3  could feel humiliated bn detachment realized n...             2\n",
       "4  Even in the 1930s the anti Semitism of literar...             0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04797fba",
   "metadata": {},
   "source": [
    "# Cleaning the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2050cad4",
   "metadata": {},
   "source": [
    "### Delete duplicate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74934013",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980cf2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3c8e5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the first 200k rows\n",
    "# df = df.head(10000)\n",
    "# df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e29a0e",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b797c971",
   "metadata": {},
   "source": [
    "- Lowercasing: Convert all text to lowercase to ensure case insensitivity.\n",
    "- Tokenization: Split the text into individual words or tokens.\n",
    "- Removing Punctuation: Remove any punctuation marks from the text.\n",
    "- Stop Word Removal: Remove common words (e.g., \"the,\" \"is,\" \"and\") that do not contribute much to the topic modeling process.\n",
    "- Lemmatization or Stemming: Reduce words to their base or root form to consolidate variations of the same word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d60572b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokennisation\n",
    "def tokenize_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "# Apply the tokenize_text() function to the 'content' column\n",
    "token_content = df['clean_content'].apply(tokenize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b9c6ac77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove stop word + any special characters + lemmatize the tokens\n",
    "def remove_noise(tokens, stop_words = ()):\n",
    "    cleaned_tokens = []\n",
    "\n",
    "    for token, tag in pos_tag(tokens):      \n",
    "        # Removing Punctuation\n",
    "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
    "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
    "        # Removing special characters and mathematical symbols\n",
    "        token = re.sub('[^a-zA-Z0-9\\s]|(\\$.+?\\$)', '', token)\n",
    "        # remove digit words\n",
    "        token = re.sub(r'\\b\\d+\\b', '', token)\n",
    "        # remove words starts with a digit\n",
    "        token = re.sub(r'\\b\\d\\w*\\b', '', token)\n",
    "\n",
    "        # Lemmatization\n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "        \n",
    "        # check if the string is not empty or a ponctuation and doesn't exist in the stop words:\n",
    "        if len(token) > 0 and token not in string.punctuation and (token.lower() not in stop_words):\n",
    "            # Lowercasing\n",
    "            cleaned_tokens.append(token.lower())\n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "604e2e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the remove noice function: \n",
    "clean_content = token_content.apply(lambda x: remove_noise(x, stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "709485ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_content'] = clean_content.apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f77007d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "clean_content    0\n",
       "sentiment_id     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16406103",
   "metadata": {},
   "source": [
    "## Save Pandas DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bd12ca6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataframe to a CSV file\n",
    "df.to_csv('aug_emotion_clean_data_v2.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6f4be84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = pd.read_csv(\"aug_emotion_clean_data_v2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4ed2e12e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_content</th>\n",
       "      <th>sentiment_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rt anxiety might represent crushing faith char...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pump interview nervous</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bos tell today vacation next week notion anoth...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>could feel humiliated bn detachment realize ne...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>even anti semitism literary figure like hilair...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       clean_content  sentiment_id\n",
       "0  rt anxiety might represent crushing faith char...             0\n",
       "1                             pump interview nervous             0\n",
       "2  bos tell today vacation next week notion anoth...             4\n",
       "3  could feel humiliated bn detachment realize ne...             2\n",
       "4  even anti semitism literary figure like hilair...             0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dff.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f89a1152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(522333, 2)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dff.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d73d76d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "clean_content    153\n",
       "sentiment_id       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_counts = dff.isnull().sum()\n",
    "missing_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4f3b5222",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Drop NAN values\n",
    "dff = dff.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0d483d56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "clean_content    0\n",
       "sentiment_id     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dff.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bce9af6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
